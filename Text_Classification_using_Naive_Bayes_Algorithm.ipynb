{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yXSgiS12uG9"
      },
      "outputs": [],
      "source": [
        "from IPython.lib.display import join\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import operator\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "path = \"20_newsgroups\"\n",
        "\n",
        "dirs = [f for f in listdir(path)]\n",
        "dirs\n",
        "\n",
        "\n",
        "\n",
        "info={} \n",
        "for folder in dirs:\n",
        "    info[folder]=[]\n",
        "    for catalog in os.listdir(os.path.join('C:\\\\Users\\\\kalya\\Desktop\\\\project\\\\ml_project_2\\\\20_newsgroups',folder)):\n",
        "        with open(os.path.join('C:\\\\Users\\\\kalya\\Desktop\\\\project\\\\ml_project_2\\\\20_newsgroups',folder,catalog),encoding='latin-1') as catalog_open:\n",
        "            info[folder].append(catalog_open.read())\n",
        "print(len(info[dirs[1]]))\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "punctuations=list(punctuation)\n",
        "stopWords=stopwords.words('english')\n",
        "stopWords=stopWords+punctuations \n",
        "\n",
        "\n",
        "\n",
        "stopWords=stopWords+['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at',\n",
        " 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during',\n",
        " 'each', 'few', 'for', 'from', 'further','had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\",\n",
        " 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\",'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself',\n",
        " \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself','no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours' 'ourselves', 'out', 'over', 'own',\n",
        " 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that',\"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \n",
        " \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', \n",
        " 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n",
        " \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\",'will', 'with', \"won't\", 'would', \"wouldn't\", \n",
        " 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'hundred', 'thousand', '1st', '2nd', '3rd',\n",
        " '4th', '5th', '6th', '7th', '8th', '9th', '10th']\n",
        "\n",
        "\n",
        "lexicon={}\n",
        "for i in range(len(info)):\n",
        "    for item in info[dirs[i]]: \n",
        "        for piece in item.split():\n",
        "            if piece.lower() not in stopWords and len(piece.lower()) >= 5:\n",
        "                if piece.lower() not in lexicon:\n",
        "                    lexicon[piece.lower()]=1\n",
        "                else:\n",
        "                    lexicon[piece.lower()]=lexicon[piece.lower()]+1\n",
        "len(lexicon)\n",
        "\n",
        "\n",
        "\n",
        "lex_sort=sorted(lexicon.items(),key=operator.itemgetter(1),reverse=True)\n",
        "feature_list=[]\n",
        "for key in lex_sort:\n",
        "    feature_list=feature_list+[key[0]]\n",
        "feature_list=feature_list[0:2000] \n",
        "\n",
        "\n",
        "\n",
        "Y=[]  \n",
        "for i in range(len(info)):\n",
        "    for item in info[dirs[i]]:\n",
        "        Y=Y+[dirs[i]]\n",
        "Y=np.array(Y)\n",
        "\n",
        "\n",
        "\n",
        "feature_table = pd.DataFrame(columns = feature_list)\n",
        "for folder in dirs: \n",
        "    for catalog in os.listdir(os.path.join('C:\\\\Users\\\\kalya\\Desktop\\\\project\\\\ml_project_2\\\\20_newsgroups',folder)):\n",
        "        feature_table.loc[len(feature_table)] = np.zeros(len(feature_list))\n",
        "        with open(os.path.join('C:\\\\Users\\\\kalya\\Desktop\\\\project\\\\ml_project_2\\\\20_newsgroups',folder,catalog),encoding='latin-1') as catalog_open:\n",
        "            for piece in catalog_open.read().split():\n",
        "                if piece.lower() in feature_list:\n",
        "                    feature_table[piece.lower()][len(feature_table)-1]=feature_table[piece.lower()][len(feature_table)-1]+ 1 \n",
        "feature_table\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "feat=feature_table.values\n",
        "feat\n",
        "x_train,x_test,y_train,y_test=train_test_split(feat,Y,random_state=0,test_size=0.80)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def nb_fit(x_train,y_train):\n",
        "    result={}\n",
        "    result[\"total_data\"]=len(y_train)\n",
        "    class_labels=set(y_train)\n",
        "    for present_names in class_labels:\n",
        "        result[present_names]={}\n",
        "        present_records=(y_train==present_names)\n",
        "        x_train_current=x_train[present_records]\n",
        "        y_train_current=y_train[present_records]\n",
        "        complete_word_set=0\n",
        "        for i in range(len(feature_list)):\n",
        "            result[present_names][feature_list[i]]=x_train_current[:,i].sum()\n",
        "            complete_word_set=complete_word_set+x_train_current[:,i].sum()\n",
        "        result[present_names][\"count_sum\"]=complete_word_set\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "def cal_prob(x,mapItemsVals,present_cat):\n",
        "    sol=np.log(mapItemsVals[present_cat][\"count_sum\"])-np.log(mapItemsVals[\"total_data\"])\n",
        "    for i in range(len(feature_list)):\n",
        "        current_word_count=mapItemsVals[present_cat][feature_list[i]]+1\n",
        "        complete_piece_num=mapItemsVals[present_cat][\"count_sum\"]+len(feature_list)\n",
        "        present_item_prob=np.log(current_word_count)-np.log(complete_piece_num)\n",
        "        for j in range(int(x[i])): \n",
        "            sol=sol+present_item_prob\n",
        "    return sol\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def oneClassPred(x,mapItemsVals):\n",
        "    best_class=-500\n",
        "    probability_optimal=-500\n",
        "    primarySprint=True\n",
        "    feasible_cat=mapItemsVals.keys()\n",
        "    for present_cat in feasible_cat:\n",
        "        if present_cat==\"total_data\":\n",
        "            continue\n",
        "        current_class_prob=cal_prob(x,mapItemsVals,present_cat)\n",
        "        if(primarySprint==True or current_class_prob>probability_optimal):\n",
        "            best_class=present_cat\n",
        "            probability_optimal=current_class_prob\n",
        "        primarySprint=False\n",
        "    return best_class\n",
        "\n",
        "\n",
        "\n",
        "def prediction(X_test,mapItemsVals):\n",
        "    Y_pred=[]\n",
        "    num = 0\n",
        "    for x in X_test:\n",
        "        Y_pred=Y_pred+[oneClassPred(x,mapItemsVals)]\n",
        "    return Y_pred\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "mapItemsVals=nb_fit(x_train,y_train)\n",
        "y_pred=prediction(x_test,mapItemsVals)\n",
        "\n",
        "\n",
        "print(confusion_matrix(y_pred,y_test))\n",
        "print(classification_report(y_pred,y_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}